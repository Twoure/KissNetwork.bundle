#!/usr/bin/env python

"""Kiss(anime, cartoon, and drama) Service code"""

# import Shared Service Code
import common as Common
import metadata as Metadata
from kissheaders import Headers
from openload import OpenloadStreamFromURL
from kissdecrypt import KissDecrypt
from preferences import Prefs
from data import Data
from io import open
import requests
import shutil
import js2py

########################################################################################
def MetadataObjectForURL(url):
    """
    This function should create and return a metadata object (for example, a
    VideoClipObject) and populate it with metadata from the given URL.
    Only the metadata should be added here, the object's key and rating_key properties
    will be synthesised based on the URL.
    """

    Log('*' * 80)
    Log(u'* MetadataObjectForURL url = {}'.format(url))
    url = Common.CorrectURL(url)

    video_id = int(url.split('=')[-1])
    show_url = url.rsplit('/', 1)[0]

    html = ElementFromURL(show_url)

    genres, genres_list = Metadata.GetGenres(html)
    date_added = Metadata.GetDateAdded(html, url)
    thumb = Callback(get_thumb, url=show_url)
    art = Metadata.get_art(url)

    # Setup MovieObject for Moives
    if 'Movie' in genres:
        Log('* -----Movie-----')

        # remove 'Movie' from genre list
        genre_list = [g for g in genres if not g == 'Movie']
        mi = Metadata.GetBaseMovieInfo(html, url)
        summary = unicode(Common.StringCode(string=mi['summary'], code='decode')) if mi['summary'] else None

        return MovieObject(
            title=mi['title'],
            genres=genre_list,
            tags=Metadata.string_to_list(Common.StringCode(string=mi['tags'], code='decode')) if mi['tags'] else [],
            source_title=mi['source_title'],
            originally_available_at=date_added if date_added else None,
            year=int(mi['year']) if mi['year'] else None,
            countries=Metadata.string_to_list(Common.StringCode(string=mi['countries'], code='decode')) if mi['countries'] else [],
            thumb=thumb,
            art=art,
            summary=summary
            )
    # Setup EpisodeObject for all shows that are not Movies
    else:
        Log('* -----TV Show-----')

        si = Metadata.GetBaseShowInfo(html, url)
        summary = unicode(Common.StringCode(string=si['summary'], code='decode')) if si['summary'] else None
        tags = Metadata.string_to_list(Common.StringCode(string=si['tags'], code='decode')) if si['tags'] else []
        show_name_raw = html.xpath('//div[@class="barContent"]/div/a[@class="bigChar"]/text()')[0]
        start_title = Metadata.get_title(html, video_id, show_name_raw)
        season_number = Metadata.GetSeasonNumber(start_title, show_name_raw, tags, summary)
        ep_name, ep_number = Metadata.GetEpisodeNameAndNumber(html, start_title, url)
        new_title = Metadata.GetEpisodeTitle(int(season_number), ep_name, int(ep_number))

        return EpisodeObject(
            title=new_title,
            source_title=si['source_title'],
            show=si['tv_show_name'],
            season=int(season_number),
            index=int(ep_number),
            tags=tags,
            originally_available_at=date_added if date_added else None,
            thumb=thumb,
            art=art,
            summary=summary
            )

########################################################################################
def MediaObjectsForURL(url):
    """
    This function should create and return a list of media objects
    and part objects representing the media available at the given URL.
    Callbacks may be used if obtaining the final media location requires
    additional computation.
    """

    Log.Debug('*' * 80)
    mo = list()
    fmt_list = [('1080p', 'm37', 5000), ('720p', 'm22', 2500), ('480p', 'm59', 1125), ('360p', 'm18', 650)]
    url = Common.CorrectURL(url)

    # create media objects for each video quality
    if Prefs['force_transcode']:
        Log.Debug('* Force Transcoding ON')
        for fmt, m, b in fmt_list:
            mo.append(MediaObject(
                parts=[PartObject(key=Callback(PlayVideo, url=url, m=m))],
                bitrate=b,
                video_resolution=fmt[:-1],
                audio_channels=2,
                optimized_for_streaming=False
                ))
    else:
        Log.Debug('* Force Trascoding OFF')
        for fmt, m, b in fmt_list:
            mo.append(MediaObject(
                parts=[PartObject(key=Callback(PlayVideo, url=url, m=m))],
                bitrate=b,
                video_resolution=fmt[:-1],
                container=Container.MP4,
                video_codec=VideoCodec.H264,
                audio_codec=AudioCodec.AAC,
                audio_channels=2,
                optimized_for_streaming=True
                ))

    return mo

########################################################################################
@indirect
def PlayVideo(url, m, **kwargs):
    """
    Get Video URL
    Currently available host: GoogleVideo, OneDrive, and OpenLoad
    GoogleVideo links have the potential for multiple resolutions links
    OneDrive & OpenLoad give only one link (the highest), so no optional resolutions
    """

    Log.Debug('*' * 80)
    Log.Debug('* Client.Product     = {}'.format(Client.Product))
    Log.Debug('* Client.Platform    = {}'.format(Client.Platform))
    Log.Debug('* Client.Version     = {}'.format(Client.Version))

    req_url = url + ('&s={}'.format(Prefs['server']) if Prefs['server'] != 'KissNetwork' else '')

    if not 'Anime' in Common.GetTypeTitle(url):
        r = NRequest(req_url, raw=True)
        headers = Headers.get_headers_for_url(url)
        dc = headers['cookie']
        headers.update({'cookie': '; '.join([dc, 'k_token={}'.format(r.cookies['k_token'])])})
        page_text = r.text
    else:
        headers = None
        page_text = NRequest(req_url)

    gtest = True if Regex(r'id\=[\"\'](selectQuality)').search(page_text) else False
    if (gtest == False) and (Prefs['server'] == 'KissNetwork'):
        page_text = NRequest(req_url + '&s=openload')

    onedrive = Regex(r'src\=[\"\'](https?\:\/\/onedrive\.live\.com\/prev\?.+?)[\"\']').search(page_text)
    openload = Regex(r'src\=[\"\'](https?\:\/\/o(?:pen)?load.+?)[\"\']').search(page_text)
    streammoe = Regex(r'src\=[\"\'](https?\:\/\/stream\.moe\/embed.+?)[\"\']').search(page_text)
    if onedrive:
        Log.Debug('* OneDrive URL       = {}'.format(onedrive.group(1)))
        vurl = get_onedrive_url(onedrive.group(1))
    elif openload:
        Log.Debug('* OpenLoad URL       = {}'.format(openload.group(1)))
        vurl = get_openload_url(openload.group(1))
    elif streammoe:
        Log.Debug('* StreamMoe URL      = {}'.format(streammoe.group(1)))
        vurl = get_streammoe_url(streammoe.group(1))
    else:
        vurl = get_googlevideo_url(page_text, url, m, headers)
    Log.Debug('* PlayVideo URL      = {}'.format(vurl))

    if Prefs['force_redirect'] and (Prefs['force_transcode'] == False) and (Prefs['server'] == 'KissNetwork'):
        Log.Debug('* Force Redirect ON')
        Log.Debug('* Note: Videos will NO longer play outside the network connection.')
        try:
            vurl = get_url_redirect_v2(vurl)
            if 'googlevideo' in vurl and not vurl == False:
                Log.Debug('* URL Redirect       = {}'.format(vurl.split('?')[0] + '...'))
            else:
                Log.Debug('* URL Redirect       = {}'.format(vurl))
        except:
            Log.Exception('* URL Redirect faild. Returning PlayVideo URL instead')
    else:
        Log.Debug('* Force Redirect OFF')

    Log.Debug('*' * 80)

    if vurl:
        return IndirectResponse(VideoClipObject, key=vurl)

    raise Ex.MediaNotAvailable

####################################################################################################
def get_googlevideo_url(page_text, url, m, headers):
    """
    Get GoogleVideo URLs
    Returns the Hights stream playable depending on the previous Stream Selections
    If Stream not found, then try's to find next hightest.
    Example 1: format list = [1080p, 720p, 360p]
    If 480p was previously chosen, then 720p will be used
    Example 2: format list = [720p, 480p, 360p]
    If 1080p was previously chosen, then 720p will be used
    """

    html = HTML.ElementFromString(page_text)
    olist = html.xpath('//select[@id="selectQuality"]/option')
    type_title = Common.GetTypeTitle(url)
    type_title_lower = type_title.lower()
    if not olist:
        Log.Error('* This Video is broken, Kiss{} is working to fix it.'.format(type_title))
        raise Ex.MediaNotAvailable

    vurl = False
    vurls = list()
    # format info taken from here:
    # https://github.com/rg3/youtube-dl/blob/fd050249afce1bcc9e7f4a127069375467007b55/youtube_dl/extractor/youtube.py#L281
    # mp4 {format: resolution} dictionary
    fmt_dict = {'37': '1080', '22': '720', '59': '480', '78': '480', '18': '360'}
    if Prefs['force_transcode']:
        # When force transcoding, then provide support for webm and flv video resolutions
        # webm {format: resolution} dictionary
        fmt_dict.update({'43': '360', '44': '480', '45': '720', '46': '1080'})
        # flv {format: resolution} dictionary
        fmt_dict.update({'35': '480', '34': '360'})
    # reversed mp4 format dictionary, paired values with resolutin selection in MediaObjectsForURL()
    rfmt_dict = {'1080': '37', '720': '22', '480': '59', '360': '18'}

    enc_test = Regex(r'\/Scripts\/(kissenc\.min\.j[^\'\"]+)').search(page_text)
    if enc_test:
        Log.Debug('* {}'.format(enc_test.group(1)))

    for node in olist:
        if enc_test:
            post_data = None
            try:
                js = Regex(r'(?s)type\=[\"\']text\/javascript[\"\']\>(.*?(window\[[^\=]+).*?)\<\/script\>').search(page_text)
                if js and (js.group(2) != 'window.location.href'):
                    ra = Regex(r'(.+window[^\=]+[^\;]+)').findall(js.group(1))
                    for s in ra:
                        jsc = js2py.EvalJs()
                        jsc.execute('var window = {};' + s)
                        if 'krsk' in jsc.window.to_dict():
                            post_data = jsc.window.to_dict()
                            break
            except:
                Log.Exception('* get_googlevideo_url Error: >>>')
            vurl_old = KissDecrypt.decrypt(node.get('value'), type_title_lower, url, headers, post_data)
        else:
            vurl_old = String.Base64Decode(node.get('value'))

        if 'googlevideo' in vurl_old:
            try:
                itag = vurl_old.split('=m')[1]
                vurls.append((vurl_old, fmt_dict[itag]))
            except:
                itag = 'No itag Found!'
                itag_test = Regex(r'itag\=(\d+)').search(vurl_old)
                if itag_test:
                    itag = str(itag_test.group(1))
                    if itag in fmt_dict.keys():
                        vurls.append((vurl_old, fmt_dict[itag]))
        else:
            try:
                itag = rfmt_dict[node.text.strip()[:-1]]
                vurls.append((vurl_old, node.text.strip()[:-1]))
            except Exception as e:
                itag = u'No itag Found: {}'.format(e)

        if not itag in fmt_dict.keys():
            Log.Warn('* Format NOT Supported: {}'.format(itag))

    if vurls:
        Log.Debug('* pre resolution selected = {}'.format(m))
        for item, mm in sorted(vurls, key=lambda tup: int(tup[1])):
            vurl = item
            nm = rfmt_dict[mm]
            if str(nm) == m[1:]:
                #Log.Debug('* Selecting {}p stream'.format(mm))
                break
            elif int(mm) > int(fmt_dict[m[1:]]):
                #Log.Debug('* Selecting {}p stream'.format(mm))
                break
        Log.Debug('* Selecting {}p stream'.format(mm))

    return vurl

####################################################################################################
def get_onedrive_url(onedrive_url):
    """
    Get OneDrive URLs
    Made by Twoure :P
    Code returns the download link for a OneDrive video
    Example Start URL
    https://onedrive.live.com/prev?cid=f90d37dd21022d17&id=F90D37DD21022D17%21108&authkey=%21AJZbMrbEbLJhvUk&parId=root&view=video&mode=interactiveEmbed
    Note the Start URL needs the 'cid', 'id', and 'authkey'
    No need to do the URL redirect, final video URL is NOT bound to local ip
    """

    html = HTML.ElementFromURL(onedrive_url)

    # Pull out the GetItemsLoaderConfing and store info in data dictionary
    for node in html.xpath('//script[@type="text/javascript"]'):
        match = Regex(r'(?s)GetItemsLoaderConfig\ \=\ (\{.*?\})\;').search(node.text_content())
        if match:
            test = match.group(1).replace('\n\r', '').strip()
            for i, s in enumerate(['cid', 'skyApiDomain', 'mkt', 'appId', 'canary', 'oauthToken', 'authKey', 'ticket', 'gb', 'rset']):
                if i == 0:
                    data = {s: Regex(r'{}\:\ \'(.*)\'\,'.format(s)).search(test).group(1)}
                elif not s == 'rset':
                    data.update({s: Regex(r'{}\:\ \'(.*)\'\,'.format(s)).search(test).group(1)})
                else:
                    data.update({s: Regex(r'{}\:\ \'(.*?)\''.format(s)).search(test).group(1)})
            break

    # parse parameters in URL
    uris = onedrive_url.split('&')
    for i, s in enumerate(uris):
        if i == 0:
            bs = s.split('?')[1]
            ss = bs.split('=')
            a = {ss[0]: ss[1]}
        else:
            ns = s.split('=')
            a.update({ns[0]: ns[1]})

    # create the request for the download url
    dl_url = 'https://onedrive.live.com/GetDownloadUrl/?cid=' + a['cid'] + '&resid=' + a['id'] + '&authkey=' + data['authKey'].decode('unicode-escape') + '&canary='
    # setup headers for request. Needs authKey and appId
    h = {'X-Requested-With': 'XMLHttpRequest', 'InvitationToken': data['authKey'].decode('unicode-escape'), 'AppId': data['appId'], 'Accept': 'application/json'}

    # open download request and parse json data
    try:
        # open download request and parse json data
        dl_data = JSON.ObjectFromURL(dl_url, headers=h)
        # remove the download info after .mp4, that way the video will play instead of force downloading
        vurl = dl_data['DownloadUrl'].split('?')[0]
    except:
        Log.Exception(u'* OneDrive Error: Cannot access {}'.format(dl_url))
        vurl = False

    return vurl

####################################################################################################
def get_openload_url(url):
    """
    Get OpenLoad URLs
    Code returns stream link for OpenLoad videos
    """

    http_headers = {'User-Agent': Common.USER_AGENT, 'Referer': url}
    ourl = OpenloadStreamFromURL(url, http_headers=http_headers)
    if ourl:
        rourl = get_url_redirect_v2(ourl, http_headers)
        return rourl
    else:
        Log.Error(u'* OpenloadStreamFromURL: cannot parse for stream \'{}\''.format(url))

    return url

####################################################################################################
def get_streammoe_url(moe_url):
    """Get Stream.moe URLs"""

    try:
        page = HTTP.Request(moe_url, cacheTime=CACHE_1MINUTE).content
    except:
        Log.Exception('* StreamMoe Error: >>>')
        return False

    r = Regex(r'contents\s*?\=\s*?atob\([\'\"]([^\'\"]+)').search(page)
    if r:
        html_text = String.Base64Decode(r.group(1))
        html = HTML.ElementFromString(html_text)

        vurl = html.xpath('//source/@src')
        if vurl:
            return vurl[0]

    return False

####################################################################################################
def get_url_redirect_v2(input_url, http_headers=None):
    """URL Redirect V2 using requests.head"""

    if not http_headers:
        http_headers = {'User-Agent': Common.USER_AGENT, 'Referer': input_url}

    r = requests.head(input_url, headers=http_headers)
    if 'location' in r.headers.keys():
        return r.headers['location']
    elif 'Location' in r.headers.keys():
        return r.headers['Location']
    else:
        Log.Debug(u'* URL Redirect: No Redirect URL for \"{}\"'.format(input_url))
        Log.Debug(u'* URL Redirect: Headers = {}'.format(r.headers))
    return input_url

####################################################################################################
def get_element_from_url(url, name, count=0, resp=False):
    """Error Handling for ServiceCode URL Requests"""

    try:
        page = requests.get(url, headers=Headers.get_headers_for_url(url))
        if (int(page.status_code) == 503) or (len(page.history) > 0):
            if count <= 1:
                count += 1
                if len(page.history) > 0:
                    type_title = Common.GetTypeTitle(url)
                    req_base_url = Regex(r'(https?\:\/\/(?:www\.)?\w+\.\w+)').search(page.url).group(1)
                    base_url = Regex(r'(https?\:\/\/(?:www\.)?\w+\.\w+)').search(url).group(1)
                    if req_base_url == base_url:
                        page = requests.get(page.url, headers=Headers.get_headers_for_url(req_base_url))
                        if Regex(r'(^The service is unavailable.$)').search(page.text):
                            Log.Warn('* The service is unavailable. Not caching \'{}\''.format(page.url))
                        elif Regex(r'\/recaptcha\/api\.js').search(page.text):
                            Log.Error(u'* Human Verification needed for \'{}\''.format(page.url))
                            Log.Warn(str(page.text))
                            return False if resp else HTML.Element('head', 'Error')
                        else:
                            Data.Save(Data.HTTP(name), page.text, resp)
                        return page if resp else HTML.ElementFromString(page.text)
                    else:
                        Log.Warn('* ServiceCode, get_element_from_url Error: HTTP 301 Redirect Error. Refreshing {} Domain'.format(type_title))
                        Log.Warn('* ServiceCode, get_element_from_url Error: page history {} | {}'.format(url, page.history))
                        Domain.UpdateDomain(type_title, True)
                        url = Common.CorrectURL(url)
                else:
                    Log.Warn('* ServiceCode, get_element_from_url Error: HTTP 503 Site Error. Refreshing site cookies')
                    Headers.get_headers_for_url(url, update=True)
                return get_element_from_url(url, name, count, resp)
            else:
                Log.Error('* ServiceCode, get_element_from_url Error: HTTP 503 Site error, tried refreshing cookies but that did not fix the issue')
                if Data.Exists(Data.HTTP(name)):
                    Log.Warn('* Using old cached page')
                    return page if resp else HTML.ElementFromString(page.text)
        else:
            try:
                page.raise_for_status()
                if Regex(r'(^The service is unavailable.$)').search(page.text):
                    Log.Warn('* The service is unavailable. Not caching \'{}\''.format(page.url))
                elif Regex(r'\/recaptcha\/api\.js').search(page.text):
                    Log.Error(u'* Human Verification needed for \'{}\''.format(page.url))
                    Log.Warn(str(page.text))
                    return False if resp else HTML.Element('head', 'Error')
                else:
                    Data.Save(Data.HTTP(name), page.text, resp)
                return page if resp else HTML.ElementFromString(page.text)
            except Exception, e:
                if (int(page.status_code) == 522):
                    Log.Error('* ServiceCode, get_element_from_url Error: HTTP 522 Site Error, site is currently offline')
                elif (int(page.status_code) == 524):
                    Log.Error('* ServiceCode, get_element_from_url Error: HTTP 524 Site Error, A timeout occurred')
                    if count < 1:
                        Log.Debug('* ReTrying \'{}\''.format(page.url))
                        count += 1
                        return get_element_from_url(url, name, count, resp)
                else:
                    Log.Error('* ServiceCode, get_element_from_url Error: Unknown Site Error, check output below.')
                Log.Error(u'* {}'.format(e))
    except:
        Log.Exception(u'* ServiceCode, get_element_from_url Error: Cannot load {}'.format(url))

    return False if resp else HTML.Element('head', 'Error')

####################################################################################################
def ElementFromURL(url):
    """setup requests html"""

    match = False
    name = Hash.MD5(url)
    path = Data.data_item_path('DataHTTP')
    Data.ensure_dirs(path)
    files = [f for f in Data.list_dir(path) if Data.file_exists(Data.join_path(path, f))]

    for filename in files:
        if filename == name:
            match = True
            if (Datetime.FromTimestamp(Data.last_modified(Data.join_path(path, filename))) + Common.TIMEOUT) <= Datetime.Now():
                Log.Debug('* Re-Caching \'{}\' to DataHTTP'.format(url))
                html = get_element_from_url(url, name)
                break
            else:
                Log.Debug('* Fetching \'{}\' from DataHTTP'.format(url))
                html = HTML.ElementFromString(Data.Load(Data.HTTP(filename)))
                break

    if not match:
        Log.Debug('* Caching \'{}\' to DataHTTP'.format(url))
        html = get_element_from_url(url, name)

    return html

####################################################################################################
def NRequest(url, raw=False):
    """setup requests text page"""

    match = False
    name = Hash.MD5(('resp_' if raw else '')+url)
    path = Data.data_item_path('DataHTTP')
    Data.ensure_dirs(path)
    files = [f for f in Data.list_dir(path) if Data.file_exists(Data.join_path(path, f))]

    for filename in files:
        if filename == name:
            match = True
            if (Datetime.FromTimestamp(Data.last_modified(Data.join_path(path, filename))) + Common.TIMEOUT) <= Datetime.Now():
                Log.Debug('* Fetching fresh data for \'{}\''.format(url))
                r = get_element_from_url(url, name, resp=raw)
                break
            else:
                Log.Debug('* Fetching \'{}\' from DataHTTP'.format(url))
                r = Data.Load(Data.HTTP(filename), raw)
                break

    if not match:
        Log.Debug('* URL Not in Cache, Fetching fresh data from \'{}\''.format(url))
        r = requests.get(url, headers=Headers.get_headers_for_url(url))
        r = r if raw else r.text
        Data.Save(Data.HTTP(name), r, raw)

    return r

########################################################################################
def get_thumb(url):
    thumb = None
    html = ElementFromURL(url)
    cover_url = Common.CorrectCoverImage(html.xpath('//head/link[@rel="image_src"]/@href')[0])

    if Common.is_kiss_url(cover_url):
        cover_file = cover_url.rsplit('/')[-1]
        type_title = Common.GetTypeTitle(url)
        if Data.CoverExists(Data.join_path(type_title, cover_file)):
            return Data.data_object(Data.Covers(Data.join_path(type_title, cover_file)))

        type_dir = Data.data_item_path(Data.Covers(type_title))
        Data.ensure_dirs(type_dir)
        path = Data.join_path(type_dir, cover_file)
        Log('* file save path \'{}\''.format(path))

        if not Data.file_exists(path):
            r = requests.get(cover_url, headers=Headers.get_headers_for_url(cover_url), stream=True)
            if r.status_code == 200:
                with open(path, 'wb') as f:
                    r.raw.decode_content = True
                    shutil.copyfileobj(r.raw, f)
                Log('* Successfully saved \'{}\' in DataCovers'.format(cover_url, path))
            else:
                Log.Error('* Save file requests status \'{}\''.format(r.status_code))
        else:
            Log.Warn('* Cover file already exists, Skipped file save.')

        if Data.CoverExists(Data.join_path(type_title, cover_file)):
            return Data.data_object(Data.Covers(Data.join_path(type_title, cover_file)))

        Log.Error('* Issues loading \'{}\''.format(path))
    elif 'http' in cover_url:
        thumb = Redirect(cover_url)
    else:
        Log.Error('*' * 80)
        Log.Error('* cover url not a valid picture url | {}'.format(cover_url))
        Log.Error('*' * 80)

    return thumb
